{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling on reddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_submissions_comments(path_submissions, path_comments, filename_comments):\n",
    "    all_files = glob.glob(os.path.join(path_submissions, 'part-*'))\n",
    "\n",
    "    dfs = pd.concat([pd.read_csv(f) for f in all_files])\n",
    "    dfs = dfs[dfs.num_comments != 0]\n",
    "\n",
    "    comment_folder = glob.glob(os.path.join(path_comments, filename_comments))\n",
    "    comments_files = [glob.glob(os.path.join(folder, \"part-*\")) for folder in comment_folder]\n",
    "    comments_files_all = [file for sublist in comments_files for file in sublist]\n",
    "\n",
    "    dfc = pd.concat([pd.read_csv(f) for f in comments_files_all])\n",
    "    \n",
    "    return dfs, dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_comments_per_submission(x):\n",
    "    return pd.Series(dict(comments=' '.join(x['body'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "def topic_modeling(text, num_topics):\n",
    "    \n",
    "    #################### Pre-processing ##################\n",
    "    # Removing numbers\n",
    "    text = [re.sub(r'\\d+', '', t) for t in text]\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = [nltk.word_tokenize(doc) for doc in text]\n",
    "    text_tokens = nltk.Text(tokens)\n",
    "    \n",
    "    # Stop words removal\n",
    "    extra_stop = set((\"''\", \",\",\":\",\"...\",\".\",\";\",\"``\",\"\\'\",\"la\",\"en\",\"le\",\"et\",\"ist\",\n",
    "                      \"das\",\"nicht\",\"ich\",\"zu\",\"du\",\"es\",\"von\",\"mit\",\"auch\",\"let\",\"man\",\n",
    "                     \"fÃ¼r\", \"den\", \"auf\", \"ein\", \"dass\", \"les\", \"que\",\"un\",\"pas\"))\n",
    "    stops = set(stopwords.words('english')).union(extra_stop)\n",
    "    filtered_text = [[word for word in t if str(word).lower() not in stops] for t in text_tokens]\n",
    "\n",
    "    # Remove very frequent and very in-frequent words\n",
    "    lower_freq = 10\n",
    "    upper_freq = 1000    \n",
    "    frequency = defaultdict(int)\n",
    "    for text in filtered_text:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    texts = [[token for token in text if frequency[token] > lower_freq and frequency[token] < upper_freq and len(token) != 1]\n",
    "             for text in filtered_text]\n",
    "    \n",
    "    # Removing documents less than minimum length\n",
    "    min_length = 30\n",
    "    long_texts = [text for text in texts if len(text)>=min_length]\n",
    "    ########################################################\n",
    "    \n",
    "    ############## Topic modeling using LDA #################\n",
    "    # Dictionary of all the words\n",
    "    dictionary = corpora.Dictionary(long_texts)\n",
    "    \n",
    "    # Vector representation of each document\n",
    "    corpus = [dictionary.doc2bow(long_text) for long_text in long_texts]\n",
    "    \n",
    "    # At this stage, dictionary contains the list of all words, each word with an unique integer id. \n",
    "    # 'corpus' contains for each document a bag of words representation (the number of occurrences of each word).\n",
    "    \n",
    "    model = models.LdaModel(corpus, num_topics, id2word=dictionary)\n",
    "    #model.print_topics()\n",
    "    ########################################################\n",
    "\n",
    "    return model, corpus, dictionary, long_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_submissions = \"../data/reddit_swiss_submissions_fullcorpus\"\n",
    "path_comments = '../data'\n",
    "filename_comments = \"reddit_swiss_comments_*\"\n",
    "\n",
    "[dfs_swiss, dfc_swiss] = concat_submissions_comments(path_submissions, path_comments, filename_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfsc_swiss = dfs_swiss.merge(dfc_swiss, how='inner', left_on='name', right_on='link_id')\n",
    "cps_swiss = dfsc_swiss.groupby(by='name_x').apply(concat_comments_per_submission)\n",
    "cps_swiss = cps_swiss.reset_index()\n",
    "\n",
    "dfs_name = pd.DataFrame(dfs_swiss[['name', 'title']])\n",
    "df_aggcomments_swiss = dfs_name.merge(cps_swiss, how='inner', left_on='name', right_on='name_x')\n",
    "del df_aggcomments_swiss['name_x']\n",
    "df_aggcomments_swiss['title_comments'] = df_aggcomments_swiss.apply(lambda x: x['title']+' '+x['comments'], axis=1)\n",
    "#df_aggcomments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_swiss = [t for t in df_aggcomments_swiss['title_comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"beautiful\" + 0.002*\"bank\" + 0.002*\"food\" + 0.002*\"hiking\" + 0.002*\"recommend\" + 0.002*\"area\" + 0.002*\"worth\" + 0.002*\"account\" + 0.002*\"post\" + 0.002*\"hike\"'),\n",
       " (1,\n",
       "  '0.002*\"Schweiz\" + 0.002*\"army\" + 0.002*\"aber\" + 0.002*\"des\" + 0.002*\"oder\" + 0.002*\"im\" + 0.002*\"als\" + 0.002*\"eine\" + 0.002*\"sind\" + 0.002*\"nur\"'),\n",
       " (2,\n",
       "  '0.003*\"change\" + 0.003*\"gold\" + 0.002*\"initiative\" + 0.002*\"government\" + 0.002*\"political\" + 0.002*\"believe\" + 0.002*\"start\" + 0.002*\"means\" + 0.002*\"read\" + 0.002*\"taxes\"'),\n",
       " (3,\n",
       "  '0.003*\"TV\" + 0.002*\"gun\" + 0.002*\"guns\" + 0.002*\"name\" + 0.002*\"love\" + 0.002*\"put\" + 0.002*\"watch\" + 0.002*\"Billag\" + 0.002*\"almost\" + 0.002*\"learn\"'),\n",
       " (4,\n",
       "  '0.002*\"Lausanne\" + 0.002*\"Basel\" + 0.002*\"friends\" + 0.002*\"company\" + 0.002*\"months\" + 0.002*\"water\" + 0.002*\"found\" + 0.002*\"CERN\" + 0.002*\"open\" + 0.002*\"visit\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 5\n",
    "[model_swiss, corpus, dictionary, long_texts] = topic_modeling(text_swiss, num_topics)\n",
    "model_swiss.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_submissions = \"../data/reddit_uk_submissions_fullcorpus\"\n",
    "path_comments = '../data'\n",
    "filename_comments = \"reddit_uk_comments_*\"\n",
    "\n",
    "[dfs_uk, dfc_uk] = concat_submissions_comments(path_submissions, path_comments, filename_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfsc_uk = dfs_uk.merge(dfc_uk, how='inner', left_on='name', right_on='link_id')\n",
    "cps_uk = dfsc_uk.groupby(by='name_x').apply(concat_comments_per_submission)\n",
    "cps_uk = cps_uk.reset_index()\n",
    "\n",
    "dfs_name = pd.DataFrame(dfs_uk[['name', 'title']])\n",
    "df_aggcomments_uk = dfs_name.merge(cps_uk, how='inner', left_on='name', right_on='name_x')\n",
    "del df_aggcomments_uk['name_x']\n",
    "df_aggcomments_uk['title_comments'] = df_aggcomments_uk.apply(lambda x: x['title']+' '+x['comments'], axis=1)\n",
    "df_aggcomments_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_uk = [t for t in df_aggcomments_uk['title_comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "[model_swiss, corpus, dictionary, long_texts] = topic_modeling(text_uk, num_topics)\n",
    "model_swiss.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_submissions = \"../data/reddit_uk_submissions_fullcorpus\"\n",
    "path_comments = '../data'\n",
    "filename_comments = \"reddit_uk_comments_*\"\n",
    "\n",
    "[dfs_eu, dfc_eu] = concat_submissions_comments(path_submissions, path_comments, filename_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "germany_subreddit_list = ['germany', 'de', 'German', 'GermanPractice', 'GermanFacts', 'GermanConversation',                  ## germany\n",
    "     'SCHLAND', 'germanyusa', 'DEjobs', 'bundesliga', 'GermanyPics', 'germusic', 'de_punk', 'germanrap', 'NDH']                 ## germany\n",
    "france_subreddit_list  = ['blagues', 'cinemacinema', 'europe', 'france', 'FrancePics', 'frenchelectro', 'Frenchhistory', 'guessthefrenchmovie',      ## france\n",
    "     'Ligue1', 'Livres', 'musiquefrancaise', 'paris', 'pedale', 'philosophie', 'Politique', 'rance', 'ScienceFr', 'SocialFrance']## france\n",
    "italy_subreddit_list  = ['Calcio', 'ITAGLIA', 'Italianhistory', 'ITALIANMUSIC', 'italy', 'ItalyPhotos', 'Libri', 'Abruzzo', 'Apulia', 'bari',       ## italy\n",
    "     'Basilicata', 'bologna', 'Calabria', 'Campania', 'Catania', 'emilia_romagna', 'firenze', 'friuli', 'Genova', 'Italia',     ## italy\n",
    "     'lazio', 'Liguria', 'lombardia', 'Lombardy', 'marche', 'messina', 'milano', 'Modena', 'molise', 'Naples_Italy', 'napoli',  ## italy\n",
    "     'padova', 'Palermo', 'Perugia', 'Piedmont', 'piemonte', 'Pisa', 'puglia', 'roma', 'rome', 'romesocialclub', 'Sardegna',    ## italy\n",
    "     'Sardinia', 'Sicilia', 'sicily', 'Siracusa', 'torino', 'Toscana', 'trentino_alto_adige', 'trentod', 'Trieste',             ## italy\n",
    "     'tuscany', 'Umbria', 'valle_daosta', 'Veneto', 'Venezia']                                                                 ## italy\n",
    "spain_subreddit_list   =  ['Barcelona', 'EPANA', 'es', 'europe', 'futbol', 'Granada', 'LaLiga', 'Madrid', 'spain', 'Andalucia', 'SpanishHistory']     ## spain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs_germany = dfs_eu[dfs_eu['subreddit'].isin(germany_subreddit_list)]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
