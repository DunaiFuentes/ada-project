{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aims to retrieve Swiss-related subreddits from the suggestions in /r/Switzerland\n",
    "\n",
    "NOTE: We could have simply copied the names from the website, as they are not an unbereable amount to be done by hand, but we thought this would be an OK warm up exercise to touch more different thing of the typical Data Analysis pipeline in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the list and Saving to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Requests' library is used to get the HTML data using a base url. 'BeautifulSoup' is then used to parse the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "form_url = 'https://www.reddit.com/r/Switzerland/'\n",
    "r = requests.get(form_url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found a problems with being mistaken by a bot because we ran this several times consecutively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AskSwitzerland', 'Basel', 'Bern', 'BielBienne', 'Buenzli', 'Frauenfeld', 'Fribourg', 'Geneva', 'Liestal', 'Luzern', 'Morcote', 'Neuchatel', 'Schaffhausen', 'SanktGallen', 'Schwiiz', 'Solothurn', 'Stans', 'Suisse', 'Thun', 'Ticino', 'Winterthur', 'Zermatt', 'Zug', 'Zurich', 'Breitling', 'CHTrees', 'FCBasel', 'MatterhornPorn', 'Migros', 'Schweiz', 'SwissArmy', 'SwissArmyKnives', 'SwissBuyers', 'SwissGaming', 'SwissGuns', 'SwissHockey', 'SwissESports', 'SwissMountainDogs', 'SwissNews', 'SwissProblems', 'SwissRap', 'SwissSuperLeague', 'SwissHistory', 'CERN', 'EPFL', 'ETHZ', 'UZH']\n"
     ]
    }
   ],
   "source": [
    "#The objective is to come up with a SubReddits' name list\n",
    "SubReddits = list()\n",
    "\n",
    "#First we define the categories we are interested in, to avoid including links related to switzerland because of\n",
    "# neighbouring countries, etc.\n",
    "list_of_interesting_categories = ['» Other general Swiss Subreddits','» Special Interest Swiss Subreddits', '» Universities and Institutions']\n",
    "\n",
    "# Then we loop in search for our links\n",
    "for bq in soup.find_all('blockquote'):\n",
    "    if bq.h3.text in list_of_interesting_categories:\n",
    "        \n",
    "        #getting all the links\n",
    "        for link in bq.find_all('a', href=True):    \n",
    "            parts = link['href'].split(\"/\")\n",
    "            if len(parts) == 3:\n",
    "                SubReddits.append(parts[2])\n",
    "    \n",
    "print(SubReddits)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We save the list to csv for later use\n",
    "np.savetxt(\"Swiss_SR.csv\", SubReddits, delimiter=\",\", fmt='%s', header='Swiss SubReddit Names')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
