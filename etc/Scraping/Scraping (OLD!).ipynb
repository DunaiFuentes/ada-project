{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aims to retrieve Swiss-related subreddits from the suggestions in /r/Switzerland\n",
    "\n",
    "NOTE: We could have simply copied the names from the website, as they are not an unbereable amount to be done by hand, but we thought this would be an OK warm up exercise to touch more different thing of the typical Data Analysis pipeline in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the list and Saving to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Requests' library is used to get the HTML data using a base url. 'BeautifulSoup' is then used to parse the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "form_url = 'https://www.reddit.com/r/Switzerland/'\n",
    "r = requests.get(form_url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found a problems with being mistaken by a bot because we ran this several times consecutively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The objective is to come up with a SubReddits' name list\n",
    "subReddits = list()\n",
    "subReddits.append('Switzerland')\n",
    "\n",
    "#First we define the categories we are interested in, to avoid including links related to switzerland because of\n",
    "# neighbouring countries, etc.\n",
    "list_of_interesting_categories = ['» Other general Swiss Subreddits','» Special Interest Swiss Subreddits', '» Universities and Institutions']\n",
    "\n",
    "# Then we loop in search for our links\n",
    "for bq in soup.find_all('blockquote'):\n",
    "    if bq.h3.text in list_of_interesting_categories:\n",
    "        \n",
    "        #getting all the links\n",
    "        for link in bq.find_all('a', href=True):    \n",
    "            parts = link['href'].split(\"/\")\n",
    "            if len(parts) == 3:\n",
    "                subReddits.append(parts[2])\n",
    "    \n",
    "print(subReddits)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We save the list to csv for later use\n",
    "np.savetxt(\"Swiss_SR.csv\", SubReddits, delimiter=\",\", fmt='%s', header='Swiss SubReddit Names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load if already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Switzerland', 'AskSwitzerland', 'Basel', 'Bern', 'BielBienne', 'Buenzli', 'Frauenfeld', 'Fribourg', 'Geneva', 'Liestal', 'Luzern', 'Morcote', 'Neuchatel', 'Schaffhausen', 'SanktGallen', 'Schwiiz', 'Solothurn', 'Stans', 'Suisse', 'Thun', 'Ticino', 'Winterthur', 'Zermatt', 'Zug', 'Zurich', 'Breitling', 'CHTrees', 'FCBasel', 'MatterhornPorn', 'Migros', 'Schweiz', 'SwissArmy', 'SwissArmyKnives', 'SwissBuyers', 'SwissGaming', 'SwissGuns', 'SwissHockey', 'SwissESports', 'SwissMountainDogs', 'SwissNews', 'SwissProblems', 'SwissRap', 'SwissSuperLeague', 'SwissHistory', 'CERN', 'EPFL', 'ETHZ', 'UZH']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "subReddits = np.loadtxt(\"Swiss_SR.csv\", delimiter=\",\", dtype=bytes).astype(str)\n",
    "subReddits = subReddits.tolist()\n",
    "print(subReddits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extract all comments from the subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will extract all the submissions and comments for each of the above subreddits. For ease of use, we use Python Reddit API Wrapper (PRAW) module to extract all the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of Reddit API requires authentication through 'Oauth'. An unique user agent is required along with a client id and secret (which is generated when an Reddit application is created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_user_agent = \"ada:dvd_ada:v1.0.0 (by /u/dk01reddit)\"\n",
    "my_client_id = \"\"\n",
    "my_client_secret = \"\"\n",
    "my_username = \"\"\n",
    "my_password = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "client_auth = requests.auth.HTTPBasicAuth(my_client_id, my_client_secret)\n",
    "post_data = {\"grant_type\": \"password\", \"username\": my_username, \"password\": my_password}\n",
    "headers = {\"User-Agent\": my_user_agent}\n",
    "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "#response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"bearer x7db0XJl42n_Q53h4n--hKIpMkw\", \"User-Agent\": my_user_agent}\n",
    "response = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=headers)\n",
    "#response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "r = praw.Reddit(user_agent=my_user_agent,\n",
    "                     client_id=my_client_id,\n",
    "                     client_secret=my_client_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once authenticated, we can get all the comments for each of the subreddits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "swiss = reddit.subreddit(subReddits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for submission in swiss.new(limit=1000):\n",
    "    i=i+1\n",
    "    \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Old version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(subReddits)):\n",
    "    sr = r.get_subreddit(subReddits[i])\n",
    "    \n",
    "    #for com in sr.get_comments():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ClientException",
     "evalue": "Required configuration setting 'client_id' missing. \nThis setting can be provided in a praw.ini file, as a keyword argument to the `Reddit` class constructor, or as an environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8f9528728b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_user_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m r.set_oauth_app_info(client_id=my_client_id,\n\u001b[1;32m      5\u001b[0m                      \u001b[0mclient_secret\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_client_secret\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dunay\\Anaconda3\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, site_name, **config_settings)\u001b[0m\n\u001b[1;32m    112\u001b[0m             if getattr(self.config, attribute) in (self.config.CONFIG_NOT_SET,\n\u001b[1;32m    113\u001b[0m                                                    None):\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequired_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient_secret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONFIG_NOT_SET\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             raise ClientException(required_message.format('client_secret') +\n",
      "\u001b[0;31mClientException\u001b[0m: Required configuration setting 'client_id' missing. \nThis setting can be provided in a praw.ini file, as a keyword argument to the `Reddit` class constructor, or as an environment variable."
     ]
    }
   ],
   "source": [
    "import praw\n",
    "r = praw.Reddit(user_agent=my_user_agent)\n",
    "\n",
    "r.set_oauth_app_info(client_id=my_client_id,\n",
    "                     client_secret=my_client_secret,\n",
    "                     redirect_uri='http://localhost:8000/')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
